{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2551e0-6354-4c65-8af4-dcc58ea3aaab",
   "metadata": {},
   "source": [
    "# S3 to Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f1dba-1b03-409b-bccd-3cce09d0ab93",
   "metadata": {},
   "source": [
    "Convert s3 files to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8eb3d8c5-91ec-479b-a497-6737bb4185f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd, json, csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "import requests\n",
    "from tenacity import retry\n",
    "\n",
    "from projects_secretes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8d879b8-c55b-4783-80fe-5fc88d03178f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the time of a collection file \n",
    "def get_time_from_fileName(fileName):\n",
    "    dateName = fileName.split(\"/\")[-1][:-5]\n",
    "    date, siteName = dateName.split(\"_\")\n",
    "\n",
    "    return (date, siteName)\n",
    "\n",
    "# check if a date is within range\n",
    "# between time_range [YYYYMMDDHH, YYYYMMDDHH], inclusive of the first item\n",
    "# exclusive of the second\n",
    "def is_date_witin_range(date, date_range):\n",
    "    # Define the date range\n",
    "    start_date = datetime.strptime(date_range[0], \"%Y%m%d%H\")\n",
    "    end_date = datetime.strptime(date_range[1],\"%Y%m%d%H\")\n",
    "    \n",
    "    # The date to check\n",
    "    date_to_check = datetime.strptime(date, \"%Y%m%d%H\")  # June 15, 2023\n",
    "    return start_date <= date_to_check < end_date\n",
    "  \n",
    "is_date_witin_range(\"2024020100\", [\"2024010100\", \"2024013123\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c757459b-ca8b-47ac-980b-6f535662b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data from s3\n",
    "def read_news_file(filekey, bucket):\n",
    "    s3 = boto3.resource('s3')\n",
    "    content = s3.Object(bucket, filekey).get()['Body'].read()\n",
    "\n",
    "    # get date and site name\n",
    "    date, siteName = get_time_from_fileName(filekey)\n",
    "\n",
    "    #  read headline, url, probability and time into a list\n",
    "    content = json.loads(content)\n",
    "    if \"articles\" in content:\n",
    "        result = []\n",
    "\n",
    "        for x in content[\"articles\"]:\n",
    "            row = {}\n",
    "            row[\"url\"] = x.get(\"url\")\n",
    "            row[\"headline\"] = x.get(\"headline\")\n",
    "            row[\"datePublished_site\"] = x.get(\"datePublished\")\n",
    "            row[\"probability\"] = x[\"metadata\"][\"probability\"]\n",
    "    \n",
    "            row[\"date_collected\"] = date\n",
    "            row[\"siteName\"] = siteName\n",
    "            result.append(row)\n",
    "\n",
    "        return (True, result)\n",
    "    else:\n",
    "        # failed collection\n",
    "        return (False, filekey)\n",
    "\n",
    "# Use ThreadPoolExecutor read files in parallel\n",
    "# wrapper function of read_news_file\n",
    "def read_files_in_parallel(bucketName, fnames):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        read_news_file_with_partial = partial(read_news_file, bucket=bucketName)\n",
    "        news_headlines = list(tqdm(executor.map(read_news_file_with_partial, fnames), total=len(fnames)))\n",
    "\n",
    "    return news_headlines\n",
    "\n",
    "# post_processsing to create a list of failed collection\n",
    "def post_processsing(result):\n",
    "    processed_result = []\n",
    "    failed_sites = []\n",
    "    for x in result:\n",
    "        if x[0]:\n",
    "            processed_result.append(x[1])\n",
    "        else:\n",
    "            failed_sites.append(x[1])\n",
    "\n",
    "    processed_result = list(chain.from_iterable(processed_result))\n",
    "    return (processed_result, failed_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "275453d2-8fc6-48ac-929a-e5cffa0d6487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15104"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domestic_fnames_feb = get_news_fileNames(domestic_bucket_name, prefix = \"current_data\", time_range = [\"2024020100\", \"2024022923\"])\n",
    "len(domestic_fnames_feb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c2acb23-4641-432c-b3bf-d1c2ff1fa430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 15104/15104 [11:37<00:00, 21.65it/s]\n"
     ]
    }
   ],
   "source": [
    "news_headlines_domestic_feb, failed_collection_domestic_feb = post_processsing(read_files_in_parallel(domestic_bucket_name, domestic_fnames_feb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fb82f0d9-9de2-475d-8860-803815c8d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domestic_news = pd.DataFrame(news_headlines_domestic_feb)\n",
    "df_domestic_news.to_parquet('data/data_domestic_news_feb.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2957eef1-f8a2-42ce-aa68-e9f611563a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the failed collection list to a CSV\n",
    "\n",
    "def save_to_csv(list, path):\n",
    "    with open(path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        # Writing each item in the list as a row\n",
    "        for item in list:\n",
    "            writer.writerow([item])\n",
    "\n",
    "save_to_csv(failed_collection_domestic_feb, \"data/collection_rate/failed_collection_domestic_feb.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
